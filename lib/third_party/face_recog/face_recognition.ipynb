{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e53a462-7dc1-4973-89aa-6c3ad59bc060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "1\n",
      "open /home/zhouj0d/Science/PID26.EWS/EWS/DigitalShadow/test_data/crowd.mp4\n",
      "Detections(xyxy=array([[1473,  446, 1509,  482]]), mask=None, confidence=None, class_id=array([0]), tracker_id=None, data={})\n",
      "Detections(xyxy=array([[1473,  446, 1509,  482]]), mask=None, confidence=None, class_id=array([0]), tracker_id=None, data={})\n",
      "Detections(xyxy=array([[1473,  446, 1509,  482]]), mask=None, confidence=None, class_id=array([0]), tracker_id=None, data={})\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 170\u001b[0m\n\u001b[1;32m    168\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[0;32m--> 170\u001b[0m     output_image, detections, texts, labels, crops \u001b[38;5;241m=\u001b[39m \u001b[43mface_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_one_cv2_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mprint\u001b[39m(detections)\n",
      "Cell \u001b[0;32mIn[46], line 53\u001b[0m, in \u001b[0;36mFaceDetector.process_one_cv2_frame\u001b[0;34m(self, frame, x1, y1, crop, extend_ratio)\u001b[0m\n\u001b[1;32m     51\u001b[0m img_height, img_width \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# _y1, _x2, _y2, _x1\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m face_locations \u001b[38;5;241m=\u001b[39m \u001b[43mface_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mnumber_of_times_to_upsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumber_of_times_to_upsample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecognition:\n\u001b[1;32m     58\u001b[0m     face_encodings \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mface_encodings(frame_rgb, face_locations)\n",
      "File \u001b[0;32m~/software/anaconda3/envs/ds/lib/python3.10/site-packages/face_recognition/api.py:121\u001b[0m, in \u001b[0;36mface_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_trim_css_to_bounds(_rect_to_css(face\u001b[38;5;241m.\u001b[39mrect), img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m _raw_face_locations(img, number_of_times_to_upsample, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_trim_css_to_bounds(_rect_to_css(face), img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_raw_face_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_times_to_upsample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/software/anaconda3/envs/ds/lib/python3.10/site-packages/face_recognition/api.py:105\u001b[0m, in \u001b[0;36m_raw_face_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cnn_face_detector(img, number_of_times_to_upsample)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mface_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_times_to_upsample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "from torchvision import datasets\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "BOUNDING_BOX_ANNOTATOR = sv.BoundingBoxAnnotator(thickness = 5)\n",
    "LABEL_ANNOTATOR = sv.LabelAnnotator(text_color=sv.Color.BLACK)\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        path = self.imgs[index][0]\n",
    "        return (img, label, path)\n",
    "\n",
    "def extend_bounding_box(x1, y1, x2, y2, extend_ratio, img_width, img_height):\n",
    "    # 计算原始 bounding box 的宽度和高度\n",
    "    box_width = x2 - x1\n",
    "    box_height = y2 - y1\n",
    "    \n",
    "    # 计算扩展后的宽度和高度\n",
    "    extended_width = box_width * extend_ratio\n",
    "    extended_height = box_height * extend_ratio\n",
    "    \n",
    "    # 计算扩展后的坐标\n",
    "    new_x1 = int(max(0, x1 - (extended_width - box_width) / 2))\n",
    "    new_y1 = int(max(0, y1 - (extended_height - box_height) / 2))\n",
    "    new_x2 = int(min(img_width, x2 + (extended_width - box_width) / 2))\n",
    "    new_y2 = int(min(img_height, y2 + (extended_height - box_height) / 2))\n",
    "    \n",
    "    return new_x1, new_y1, new_x2, new_y2\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self, recognition = False, user_face_reference_img_path = None, model = None, tolerance = None):\n",
    "        self.face_embedding = None\n",
    "        self.user_face_reference_img_path = user_face_reference_img_path\n",
    "        self.number_of_times_to_upsample = 1\n",
    "        self.recognition = recognition\n",
    "        self.model = model\n",
    "        self.tolerance = tolerance\n",
    "        if self.recognition:\n",
    "            self.load_embedding()\n",
    "\n",
    "    def process_one_cv2_frame(self, frame, x1 = 0, y1 = 0, crop = True, extend_ratio = None):\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(frame_rgb)\n",
    "        img_height, img_width = frame.shape[:2]\n",
    "        # _y1, _x2, _y2, _x1\n",
    "        face_locations = face_recognition.face_locations(frame_rgb,\n",
    "                                                         number_of_times_to_upsample=self.number_of_times_to_upsample,\n",
    "                                                         model=self.model)\n",
    "\n",
    "        if self.recognition:\n",
    "            face_encodings = face_recognition.face_encodings(frame_rgb, face_locations)\n",
    "            face_names = []\n",
    "            known_face_encodings, known_face_names = self.face_embedding_data[0], self.face_embedding_data[1]\n",
    "            for face_encoding in face_encodings:\n",
    "                # See if the face is a match for the known face(s)\n",
    "                matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance = self.tolerance)\n",
    "                name = \"NA\"\n",
    "\n",
    "                # # If a match was found in known_face_encodings, just use the first one.\n",
    "                # if True in matches:\n",
    "                #     first_match_index = matches.index(True)\n",
    "                #     name = known_face_names[first_match_index]\n",
    "\n",
    "                # Or instead, use the known face with the smallest distance to the new face\n",
    "                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                best_match_index = numpy.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                face_names.append(name)\n",
    "\n",
    "        new_face_locations = []\n",
    "        if len(face_locations) > 0:\n",
    "            for _ in face_locations:\n",
    "                (_y1, _x2, _y2, _x1) = _\n",
    "                if extend_ratio:\n",
    "                    new_x1, new_y1, new_x2, new_y2 = extend_bounding_box(_x1, _y1, _x2, _y2, extend_ratio, img_width, img_height)\n",
    "                    new_face_locations.append((new_x1, new_y1, new_x2, new_y2))\n",
    "                else:\n",
    "                    new_face_locations.append((_x1, _y1, _x2, _y2))\n",
    "        \n",
    "        if self.recognition:\n",
    "            face_names = [f'face_name@{_}' for _ in face_names]\n",
    "        else:\n",
    "            face_names = ['face_name@NA' for _ in range(len(face_locations))]\n",
    "        \n",
    "        detections = None\n",
    "        labels = face_names\n",
    "        texts = None\n",
    "        crops = []\n",
    "        \n",
    "        if len(new_face_locations) > 0:\n",
    "            class_ids = []\n",
    "            for _ in range(len(new_face_locations)):\n",
    "                class_ids.append(0)\n",
    "            detections = sv.Detections(xyxy = np.array(new_face_locations), class_id = np.array(class_ids))\n",
    "            if crop:\n",
    "                for xyxy in detections.xyxy:\n",
    "                    cropped_image = image.crop(xyxy)\n",
    "                    crops.append(cropped_image)\n",
    "                \n",
    "            image = np.array(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            image = BOUNDING_BOX_ANNOTATOR.annotate(image, detections)\n",
    "            #print(detections,labels)\n",
    "            image = LABEL_ANNOTATOR.annotate(image, detections, labels=labels)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(image)\n",
    "                \n",
    "        return image, detections, texts, labels, crops\n",
    "\n",
    "    def load_embedding(self):\n",
    "        if os.path.isfile('{}/face_embedding_data.pt'.format(self.user_face_reference_img_path)) == False:\n",
    "            print('>> generate face embedding')\n",
    "            dataset = ImageFolderWithPaths(self.user_face_reference_img_path)  # photos folder path\n",
    "            idx_to_class = {i: c for c, i in\n",
    "                            dataset.class_to_idx.items()}  # accessing names of peoples from folder names\n",
    "\n",
    "            def collate_fn(x):\n",
    "                return x[0]\n",
    "\n",
    "            loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "            known_face_encodings = []  # list of names corrospoing to cropped photos\n",
    "            known_face_names = []  # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
    "\n",
    "            for img, idx, path in loader:\n",
    "                print(path)\n",
    "                image = face_recognition.load_image_file(path)\n",
    "                encoding = face_recognition.face_encodings(image)[0]\n",
    "                known_face_encodings.append(encoding)\n",
    "                known_face_names.append(idx_to_class[idx])\n",
    "\n",
    "            data = [known_face_encodings, known_face_names]\n",
    "            torch.save(data, '{}/face_embedding_data.pt'.format(self.user_face_reference_img_path))  # saving data.pt file\n",
    "            load_data = data\n",
    "        else:\n",
    "            print('>> face embedding exist, loading')\n",
    "            load_data = torch.load('{}/face_embedding_data.pt'.format(self.user_face_reference_img_path))\n",
    "        self.face_embedding_data = load_data\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import dlib\n",
    "    print(dlib.DLIB_USE_CUDA) # test CUDA support\n",
    "    print(dlib.cuda.get_num_devices())\n",
    "    \n",
    "    face_detector = FaceDetector(recognition=False, user_face_reference_img_path='../../user_profile')\n",
    "    video_path = '/home/zhouj0d/Science/PID26.EWS/EWS/DigitalShadow/test_data/crowd.mp4'\n",
    "\n",
    "    try:\n",
    "        # 打开摄像头\n",
    "        cap = cv2.VideoCapture(int(float(video_path)))\n",
    "        print(f'open camera {video_path}')\n",
    "    except:\n",
    "        # 打开视频文件\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        print(f'open {video_path}')\n",
    "\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            output_image, detections, texts, labels, crops = face_detector.process_one_cv2_frame(frame = frame)\n",
    "            print(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d25af-4eac-40fa-8ab7-50e1d06b63c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
